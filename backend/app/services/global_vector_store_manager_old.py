"""
Global Vector Store Manager - Manages a single global vector store for all admin documents
"""
import os
import json
from typing import List, Dict, Optional, Tuple
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from loguru import logger

from ..services.dual_embedding_manager import EmbeddingManager
from ..services.admin_document_service import GlobalVectorStoreService
from ...database_connection import get_db_connection

class GlobalVectorStoreManager:
    """Manager for the single global vector store containing all admin documents"""
    
    def __init__(self):
        self.vector_store_dir = os.path.join(
            os.path.dirname(__file__), 
            '../../vector_stores/admin_documents'
        )
        os.makedirs(self.vector_store_dir, exist_ok=True)
        self.global_store_path = os.path.join(self.vector_store_dir, "global_knowledge_base")
        
        # Ensure document_chunks table exists
        GlobalVectorStoreService.create_document_chunks_table()
    
    def _load_or_create_global_store(self) -> FAISS:
        """Load existing global store or create new empty one"""
        try:
            embeddings = EmbeddingManager.get_embeddings_static()
            
            # Check if global store exists
            index_file = os.path.join(self.global_store_path, "index.faiss")
            pkl_file = os.path.join(self.global_store_path, "index.pkl")
            
            if os.path.exists(index_file) and os.path.exists(pkl_file):
                # Load existing store
                vectorstore = FAISS.load_local(
                    self.global_store_path,
                    embeddings,
                    index_name="index",
                    allow_dangerous_deserialization=True
                )
                logger.info(f"Loaded existing global vector store with {vectorstore.index.ntotal} vectors")
                return vectorstore
            else:
                # Create new empty store
                logger.info("Creating new global vector store")
                # Create with a dummy document that we'll remove
                dummy_texts = ["Initializing global vector store"]
                dummy_metadatas = [{"document": "system", "temporary": True}]
                
                vectorstore = FAISS.from_texts(
                    dummy_texts,
                    embeddings,
                    metadatas=dummy_metadatas
                )
                
                # Save the empty store
                vectorstore.save_local(self.global_store_path, index_name="index")
                logger.info("Created new global vector store")
                return vectorstore
                
        except Exception as e:
            logger.error(f"Error loading/creating global vector store: {e}")
            raise\n    \n    def add_document_to_global_store(self, document_id: int, chunks_with_metadata: List[Dict]) -> bool:\n        \"\"\"Add a document's chunks directly to the global vector store\"\"\"\n        try:\n            if not chunks_with_metadata:\n                logger.warning(f\"No chunks provided for document {document_id}\")\n                return False\n            \n            # Load the global store\n            global_store = self._load_or_create_global_store()\n            current_vector_count = global_store.index.ntotal\n            \n            # Add document metadata to each chunk\n            enriched_chunks = []\n            texts = []\n            metadatas = []\n            \n            for i, chunk_data in enumerate(chunks_with_metadata):\n                # Enrich metadata with document information\n                metadata = chunk_data.get('metadata', {}).copy()\n                metadata['document_id'] = document_id\n                metadata['chunk_index'] = i\n                metadata['global_chunk_id'] = f\"doc_{document_id}_chunk_{i}\"\n                \n                enriched_chunks.append({\n                    'text': chunk_data['text'],\n                    'metadata': metadata\n                })\n                \n                texts.append(chunk_data['text'])\n                metadatas.append(metadata)\n            \n            # Add new texts to the global store\n            embeddings = EmbeddingManager.get_embeddings_static()\n            new_vectors = embeddings.embed_documents(texts)\n            \n            # Add vectors to FAISS index\n            global_store.add_texts(texts, metadatas=metadatas)\n            \n            # Save the updated global store\n            global_store.save_local(self.global_store_path, index_name=\"index\")\n            \n            logger.info(f\"Added {len(chunks_with_metadata)} chunks from document {document_id} to global store\")\n            logger.info(f\"Global store now has {global_store.index.ntotal} total vectors\")\n            \n            # Track chunks in database\n            success = GlobalVectorStoreService.add_document_chunks(\n                document_id, \n                enriched_chunks, \n                current_vector_count\n            )\n            \n            if not success:\n                logger.error(f\"Failed to track chunks in database for document {document_id}\")\n                return False\n            \n            # Update global vector store record\n            GlobalVectorStoreService.add_document_to_global_store(\n                document_id,\n                self.global_store_path,\n                len(chunks_with_metadata)\n            )\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error adding document {document_id} to global store: {e}\")\n            return False\n    \n    def remove_document_from_global_store(self, document_id: int) -> bool:\n        \"\"\"Remove a document's chunks from the global vector store\"\"\"\n        try:\n            # Get chunk information\n            removal_info = GlobalVectorStoreService.remove_document_from_global_store(document_id)\n            \n            if removal_info['removed_chunks'] == 0:\n                logger.warning(f\"No chunks found to remove for document {document_id}\")\n                return True\n            \n            logger.info(f\"Marked {removal_info['removed_chunks']} chunks as inactive for document {document_id}\")\n            \n            # Rebuild the global vector store without inactive chunks\n            success = self._rebuild_global_store()\n            \n            if success:\n                logger.success(f\"Successfully removed document {document_id} from global store\")\n            else:\n                logger.error(f\"Failed to rebuild global store after removing document {document_id}\")\n            \n            return success\n            \n        except Exception as e:\n            logger.error(f\"Error removing document {document_id} from global store: {e}\")\n            return False\n    \n    def _rebuild_global_store(self) -> bool:\n        \"\"\"Rebuild the global vector store using only active chunks\"\"\"\n        try:\n            logger.info(\"Rebuilding global vector store with only active chunks...\")\n            \n            # Get all active chunks\n            active_chunks = GlobalVectorStoreService.get_active_document_chunks()\n            \n            if not active_chunks:\n                logger.info(\"No active chunks found, creating empty global store\")\n                # Create empty store\n                embeddings = EmbeddingManager.get_embeddings_static()\n                dummy_texts = [\"Empty global vector store\"]\n                dummy_metadatas = [{\"document\": \"system\", \"temporary\": True}]\n                \n                empty_store = FAISS.from_texts(dummy_texts, embeddings, metadatas=dummy_metadatas)\n                empty_store.save_local(self.global_store_path, index_name=\"index\")\n                \n                return True\n            \n            # Rebuild from active chunks\n            texts = []\n            metadatas = []\n            \n            for chunk in active_chunks:\n                texts.append(chunk['chunk_text'])\n                \n                # Parse metadata from JSONB\n                metadata = chunk.get('metadata', {})\n                if isinstance(metadata, str):\n                    try:\n                        metadata = json.loads(metadata)\n                    except:\n                        metadata = {}\n                \n                # Ensure document_id is in metadata\n                metadata['document_id'] = chunk['document_id']\n                metadata['chunk_index'] = chunk['chunk_index']\n                metadata['global_chunk_id'] = f\"doc_{chunk['document_id']}_chunk_{chunk['chunk_index']}\"\n                metadata['filename'] = chunk.get('filename', 'Unknown')\n                \n                metadatas.append(metadata)\n            \n            # Create new global store\n            embeddings = EmbeddingManager.get_embeddings_static()\n            new_global_store = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n            \n            # Save the rebuilt store\n            new_global_store.save_local(self.global_store_path, index_name=\"index\")\n            \n            # Update vector indices in database\n            self._update_chunk_vector_indices(active_chunks)\n            \n            logger.success(f\"Rebuilt global vector store with {len(active_chunks)} active chunks\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error rebuilding global vector store: {e}\")\n            return False\n    \n    def _update_chunk_vector_indices(self, chunks: List[Dict]) -> bool:\n        \"\"\"Update vector indices in database after rebuild\"\"\"\n        try:\n            with get_db_connection() as conn:\n                cursor = conn.cursor()\n                \n                for i, chunk in enumerate(chunks):\n                    cursor.execute(\"\"\"\n                        UPDATE document_chunks \n                        SET vector_index = %s \n                        WHERE id = %s\n                    \"\"\", (i, chunk['id']))\n                \n                conn.commit()\n                logger.info(f\"Updated vector indices for {len(chunks)} chunks\")\n                return True\n                \n        except Exception as e:\n            logger.error(f\"Error updating chunk vector indices: {e}\")\n            return False\n    \n    def get_global_store_stats(self) -> Dict:\n        \"\"\"Get statistics about the global vector store\"\"\"\n        try:\n            # Get vector store info\n            index_file = os.path.join(self.global_store_path, \"index.faiss\")\n            vector_count = 0\n            \n            if os.path.exists(index_file):\n                global_store = self._load_or_create_global_store()\n                vector_count = global_store.index.ntotal\n            \n            # Get database stats\n            chunk_count = GlobalVectorStoreService.get_global_chunk_count()\n            \n            with get_db_connection() as conn:\n                cursor = conn.cursor()\n                \n                # Get document count\n                cursor.execute(\"\"\"\n                    SELECT COUNT(DISTINCT document_id) \n                    FROM document_chunks dc\n                    JOIN admin_documents ad ON dc.document_id = ad.id\n                    WHERE dc.is_active = true AND ad.is_active = true\n                \"\"\")\n                doc_count = cursor.fetchone()[0] or 0\n            \n            return {\n                'total_vectors': vector_count,\n                'total_chunks': chunk_count,\n                'total_documents': doc_count,\n                'store_path': self.global_store_path,\n                'store_exists': os.path.exists(index_file)\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error getting global store stats: {e}\")\n            return {\n                'total_vectors': 0,\n                'total_chunks': 0,\n                'total_documents': 0,\n                'store_path': self.global_store_path,\n                'store_exists': False,\n                'error': str(e)\n            }\n    \n    def get_document_list(self) -> List[Dict]:\n        \"\"\"Get list of all documents in the global store\"\"\"\n        try:\n            with get_db_connection() as conn:\n                cursor = conn.cursor()\n                \n                cursor.execute(\"\"\"\n                    SELECT DISTINCT \n                        ad.id,\n                        ad.filename,\n                        ad.original_filename,\n                        ad.file_size,\n                        ad.created_at,\n                        ad.language,\n                        COUNT(dc.id) as chunk_count\n                    FROM admin_documents ad\n                    JOIN document_chunks dc ON ad.id = dc.document_id\n                    WHERE ad.is_active = true AND dc.is_active = true\n                    GROUP BY ad.id, ad.filename, ad.original_filename, ad.file_size, ad.created_at, ad.language\n                    ORDER BY ad.created_at DESC\n                \"\"\")\n                \n                documents = cursor.fetchall()\n                return [dict(doc) for doc in documents]\n                \n        except Exception as e:\n            logger.error(f\"Error getting document list: {e}\")\n            return []\n    \n    def rebuild_entire_global_store(self) -> bool:\n        \"\"\"Completely rebuild the global store (maintenance operation)\"\"\"\n        try:\n            logger.info(\"Starting complete rebuild of global vector store...\")\n            \n            # Remove existing store files\n            if os.path.exists(self.global_store_path):\n                import shutil\n                shutil.rmtree(self.global_store_path)\n                logger.info(\"Removed existing global store files\")\n            \n            # Rebuild from database\n            success = self._rebuild_global_store()\n            \n            if success:\n                logger.success(\"Complete rebuild of global vector store successful\")\n            else:\n                logger.error(\"Failed to rebuild global vector store\")\n            \n            return success\n            \n        except Exception as e:\n            logger.error(f\"Error in complete rebuild: {e}\")\n            return False\n